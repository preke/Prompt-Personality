{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9069e364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8229\n",
      "1433\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "labelword_list = []\n",
    "for trait in ['A','C','E', 'N', 'O']:\n",
    "    f_word = open(trait+'_words.txt', 'r')\n",
    "    labelword_list += f_word.readline().split(',')\n",
    "    labelword_list += f_word.readline().split(',')\n",
    "    f_word.close()\n",
    "print(len(labelword_list))\n",
    "labelword_set = set(labelword_list)\n",
    "print(len(labelword_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5907a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "data_path = [\n",
    "    '../data/FriendsPersona/Friends_A_whole.tsv',\n",
    "    '../data/myPersonality/MyPersonality_A_whole.tsv',\n",
    "    '../data/pan2015/Pan_A_whole.tsv',\n",
    "    '../data/Essay/Essay_A_whole.tsv',\n",
    "    '../data/Kaggle_mbti/mbti_I_whole.tsv',\n",
    "    \n",
    "]\n",
    "\n",
    "for data in data_path:\n",
    "    df = pd.read_csv(data,  sep='\\t')\n",
    "    df['utterance'] = df['utterance'].apply(lambda x: x.lower())\n",
    "\n",
    "    def sent_tokenize(df,sent_method):\n",
    "        sents = []\n",
    "        for index,row in df.iterrows():\n",
    "            sent = []\n",
    "            tempSent = row['utterance'].split('|||')\n",
    "            if sent_method == 'nltk':\n",
    "                for i in tempSent:\n",
    "                    sent = sent + nltk.sent_tokenize(i)\n",
    "            elif sent_method == 'comma':\n",
    "                for i in tempSent:\n",
    "                    sent = sent + re.split(r\"[,,.,?,!,;]\",i)\n",
    "            sents = sents + sent\n",
    "        return sents\n",
    "\n",
    "    sents = sent_tokenize(df,'comma')\n",
    "    df_sents = pd.DataFrame(sents)\n",
    "\n",
    "    # tokenize function\n",
    "    def text_tokenize(text,tokenize_method):\n",
    "        # args: \n",
    "        #   text: the text need to be tokenize\n",
    "        #   tokenize_method: method use for tokenize\n",
    "        ltext = text.lower() # to lowercase\n",
    "        translab = str.maketrans(punctuation,' '*len(punctuation))\n",
    "        text_without_punctuation = ltext.translate(translab) # remove punctuation\n",
    "        if tokenize_method == 'nltk':\n",
    "            word_split = nltk.word_tokenize(text_without_punctuation) # word tokenize\n",
    "        return word_split\n",
    "\n",
    "    # generate tokens\n",
    "    import nltk\n",
    "    tokens = []\n",
    "    for sent in sents:\n",
    "        token = text_tokenize(sent,'nltk')\n",
    "        tokens.append(token)\n",
    "\n",
    "\n",
    "    df_processed = pd.DataFrame(columns=['sent_id','sentence','label_word'])\n",
    "    for index,item in enumerate(tokens):\n",
    "        for i in item:\n",
    "            if i in label_set:\n",
    "                df_processed = df_processed.append({'sent_id':'sent_'+str(index), 'sentence':sents[index], 'label_word':i},ignore_index=True)\n",
    "\n",
    "\n",
    "    def get_span(pos, row):\n",
    "        if pos == 1:\n",
    "            return row['sentence'].split(row['label_word'])[0]\n",
    "        elif pos == 2:\n",
    "            try:\n",
    "                return row['sentence'].split(row['label_word'])[1]\n",
    "            except:\n",
    "                return \"\"\n",
    "\n",
    "    df_processed['span_1'] = df_processed.apply(lambda x: get_span(1, x), axis=1)\n",
    "    df_processed['span_2'] = df_processed.apply(lambda x: get_span(2, x), axis=1)\n",
    "\n",
    "    df_processed.to_csv(data.split('_')[0]+'_map_label_words_comma.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "41708163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 5)\n",
      "(605, 5)\n",
      "(1302, 5)\n",
      "(1907, 5)\n",
      "(81, 5)\n",
      "(1988, 5)\n",
      "(7701, 5)\n",
      "(9689, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6049/649521047.py:14: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  df = pd.concat([df, tmp_df], 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label_word</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sent_94</td>\n",
       "      <td>any kind of alcohol will do</td>\n",
       "      <td>kind</td>\n",
       "      <td>MyPersonality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sent_158</td>\n",
       "      <td>(Gracias a Sofi por la canci�n tan genial</td>\n",
       "      <td>genial</td>\n",
       "      <td>MyPersonality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sent_203</td>\n",
       "      <td>&lt;3might have to trade in bastille day celebrat...</td>\n",
       "      <td>quiet</td>\n",
       "      <td>MyPersonality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sent_238</td>\n",
       "      <td>is becoming sentimental as I work on the notes...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>MyPersonality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sent_258</td>\n",
       "      <td>kind of likes getting up early and getting mor...</td>\n",
       "      <td>kind</td>\n",
       "      <td>MyPersonality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   sent_id                                           sentence  \\\n",
       "0           0   sent_94                        any kind of alcohol will do   \n",
       "1           1  sent_158          (Gracias a Sofi por la canci�n tan genial   \n",
       "2           2  sent_203  <3might have to trade in bastille day celebrat...   \n",
       "3           3  sent_238  is becoming sentimental as I work on the notes...   \n",
       "4           4  sent_258  kind of likes getting up early and getting mor...   \n",
       "\n",
       "    label_word         source  \n",
       "0         kind  MyPersonality  \n",
       "1       genial  MyPersonality  \n",
       "2        quiet  MyPersonality  \n",
       "3  sentimental  MyPersonality  \n",
       "4         kind  MyPersonality  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_file_name = [\n",
    "    './data/MyPersonality_map_label_words_comma.tsv',\n",
    "    './data/Pan_map_label_words_comma.tsv',\n",
    "    './data/Friends_map_label_words_comma.tsv',\n",
    "    './data/Essay_map_label_words_comma.tsv',\n",
    "]\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "for name in template_file_name:\n",
    "    tmp_df = pd.read_csv(name, sep='\\t')\n",
    "    tmp_df['source'] = name.split('_')[0].split('/')[-1] \n",
    "    print(tmp_df.shape)\n",
    "    df = pd.concat([df, tmp_df], 0)\n",
    "    print(df.shape)\n",
    "\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "14204484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='label_word').to_csv('total_comma_templates.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8cce93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    9689.000000\n",
      "mean       50.265456\n",
      "std       149.786865\n",
      "min         0.000000\n",
      "25%        12.000000\n",
      "50%        24.000000\n",
      "75%        48.000000\n",
      "max      3504.000000\n",
      "Name: len_span_1, dtype: float64\n",
      "\n",
      "count    9689.000000\n",
      "mean       41.400970\n",
      "std       135.368033\n",
      "min         0.000000\n",
      "25%         4.000000\n",
      "50%        16.000000\n",
      "75%        40.000000\n",
      "max      2961.000000\n",
      "Name: len_span_2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['len_span_1'] = df['span_1'].apply(len)\n",
    "df['len_span_2'] = df['span_2'].apply(len)\n",
    "\n",
    "print(df['len_span_1'].describe())\n",
    "print()\n",
    "print(df['len_span_2'].describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
