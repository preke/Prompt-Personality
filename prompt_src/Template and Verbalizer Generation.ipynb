{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce3935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6610f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "personality = 'A'\n",
    "df_data = pd.read_csv('data/Friends_'+personality+'_whole.tsv', sep = '\\t')\n",
    "df = df_data[['utterance', 'labels']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED=42\n",
    "\n",
    "df_train, df_valid, label_train, label_valid = \\\n",
    "            train_test_split(df, df['labels'], test_size=0.1, random_state=SEED, stratify=df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91204fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "class DF_Processor(DataProcessor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.labels = ['0', '1']\n",
    "\n",
    "    def get_examples(self, df):\n",
    "        examples = []\n",
    "        for i,r in df.iterrows():\n",
    "            text_a = r['utterance']\n",
    "            label = r['labels']\n",
    "            guid = i\n",
    "            example = InputExample(guid=guid, text_a=text_a, label=label)\n",
    "            examples.append(example)\n",
    "        return examples\n",
    "\n",
    "data_train = DF_Processor().get_examples(df_train)\n",
    "data_valid = DF_Processor().get_examples(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e09608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model...\n"
     ]
    }
   ],
   "source": [
    "print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-base\")\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = \\\n",
    "                load_plm('t5', 't5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad8b5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': \"  Uh, you left out the stupid part.  I think it's totally insane, I mean, they work for the hospital. It's like returning to the scene of the crime. You know, I say we blow off the dates.\", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' naive', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 706, 'label': 1}]\n"
     ]
    }
   ],
   "source": [
    "df_verbalizer = pd.read_csv('big_five_cleaned.tsv', sep='\\t')\n",
    "pos = [a.lower() for a in list(df_verbalizer['word'][df_verbalizer[personality]>0])]\n",
    "neg = [a.lower() for a in list(df_verbalizer['word'][df_verbalizer[personality]<0])]\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "classes = [0,1]        \n",
    "verbalizer = ManualVerbalizer(\n",
    "    classes = classes,\n",
    "    label_words = {\n",
    "        0 : neg, \n",
    "        1 : pos\n",
    "    },\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, \n",
    "                                           verbalizer=verbalizer, \n",
    "                                           text='{\"placeholder\":\"text_a\"} {\"mask\"} {\"meta\":\"labelword\"} {\"mask\"}.')\n",
    "\n",
    "wrapped_example = template.wrap_one_example(data_train[15])\n",
    "print(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc708b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # whether to perform automatic template generation\n",
    "auto_v = True # whether to perform automatic label word generation\n",
    "\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(10):\n",
    "        train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "    return best_score\n",
    "\n",
    "\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = model(inputs)\n",
    "            labels = inputs['label']\n",
    "            alllabels.extend(labels.cpu().tolist())\n",
    "            allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdbcfc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 230it [00:00, 1137.91it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "tokenizing: 639it [00:00, 1164.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass!\n",
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 18/18 [00:30<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<extra_id_0>', '▁I', \"'\", 'm', '<extra_id_1>', '.', '<extra_id_2>'], ['<extra_id_0>', '▁It', \"'\", 's', '<extra_id_1>', '.', '<extra_id_2>']]\n",
      "['{\"placeholder\": \"text_a\"} I\\'m {\"mask\"} ..', '{\"placeholder\": \"text_a\"} It\\'s {\"mask\"} ..']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 145it [00:00, 1449.21it/s]\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "tokenizing: 310it [00:00, 1566.38it/s]\u001b[A\n",
      "tokenizing: 639it [00:00, 1670.72it/s]\u001b[A\n",
      "\n",
      "tokenizing: 72it [00:00, 1829.21it/s]\n",
      "/home/zhiyuan/ENTER/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 50%|██████████████████████                      | 1/2 [10:05<10:05, 605.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.5694444444444444\n",
      "template: {\"placeholder\": \"text_a\"} I'm {\"mask\"} ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 200it [00:00, 1997.35it/s]\u001b[A\n",
      "tokenizing: 412it [00:00, 2068.33it/s]\u001b[A\n",
      "tokenizing: 639it [00:00, 2060.91it/s]\u001b[A\n",
      "\n",
      "tokenizing: 72it [00:00, 2104.49it/s]\n",
      "/home/zhiyuan/ENTER/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 2/2 [20:14<00:00, 607.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"placeholder\": \"text_a\"} I'm {\"mask\"} ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from tqdm import tqdm\n",
    "# template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=2) # beam_width is set to 5 here for efficiency, to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(data_train, template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(data_train), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # register all data at once\n",
    "    print('pass!')\n",
    "    \n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval() ## 不更新参数\n",
    "    print('generating...')\n",
    "    \n",
    "    template_texts = template_generator._get_templates()\n",
    "    print(template_texts)\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    # iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        template = ManualTemplate(tokenizer, template_text)\n",
    "\n",
    "        train_dataloader = PromptDataLoader(data_train, template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "        valid_dataloader = PromptDataLoader(data_valid, template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        if score > best_metrics:\n",
    "            print('best score:', score)\n",
    "            print('template:', template_text)\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "    # use the best template\n",
    "    template = ManualTemplate(tokenizer, text=best_template_text)\n",
    "    print(best_template_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a67a25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 639it [00:00, 2045.14it/s]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 196it [00:00, 1955.28it/s]\u001b[A\n",
      "tokenizing: 406it [00:00, 2038.66it/s]\u001b[A\n",
      "tokenizing: 639it [00:00, 2038.99it/s]\u001b[A\n",
      "\n",
      "tokenizing: 72it [00:00, 2080.52it/s]\n",
      "/home/zhiyuan/ENTER/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 20%|████████▊                                   | 1/5 [09:59<39:59, 599.93s/it]\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 198it [00:00, 1975.19it/s]\u001b[A\n",
      "tokenizing: 407it [00:00, 2038.45it/s]\u001b[A\n",
      "tokenizing: 639it [00:00, 2037.56it/s]\u001b[A\n",
      "\n",
      "tokenizing: 72it [00:00, 2093.29it/s]\n",
      "/home/zhiyuan/ENTER/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 20%|████████▊                                   | 1/5 [10:34<42:16, 634.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16222/4012668299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16222/1885074797.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_dataloader, val_dataloader, loss_func, optimizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16222/1885074797.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_dataloader, loss_func, optimizer)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/lib/python3.9/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # load generation model for template generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=5, label_word_num_per_class=5)\n",
    "    # to improve performance , try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(data_train, template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(data_train, template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "        valid_dataloader = PromptDataLoader(data_valid, template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "    # use the best verbalizer\n",
    "    print(best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
